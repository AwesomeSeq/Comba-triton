{ "attn": {
    "layers": [1,3,5,7,9,11,13,15,17,19,21,23],
    "num_heads": 4,
    "num_kv_heads": 256,
    "qkv_bias": false,
    "rope_theta": 10000.0,
    "window_size": 2048
  },
  "expand_v": 1,
  "use_gate": true,
  "fuse_cross_entropy": true,
  "fuse_norm": true,
  "hidden_size": 1024,
  "model_type": "comba",
  "num_heads": 4,
  "head_dim": 256,
  "initializer_range": 0.006,
  "num_hidden_layers": 24,
  "A_type": "iplr",
  "qk_sim": true,
  "tie_word_embeddings": true}